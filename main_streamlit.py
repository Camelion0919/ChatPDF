__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_classic.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_classic.callbacks.base import BaseCallbackHandler
from langchain_classic import hub
import streamlit as st
import tempfile
import os
# from dotenv import load_dotenv
# load_dotenv() # ê°œì¸ key
# api_key = os.getenv("OPENAI_API_KEY")

# upload ëœ file ë¶ˆëŸ¬ì˜¤ê¸°
def pdf_to_document(uploaded_file):
    #ì„ì‹œí´ë” ìƒì„±
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)

    with open(temp_filepath, 'wb') as f:
        f.write(uploaded_file.getvalue())

    #ì—…ë¡œë“œ ëœ íŒŒì¼ì„ document ê°ì²´ë¡œ get
    loader = PyPDFLoader(temp_filepath)
    pages = loader.load_and_split
    return pages

#title
st.title('ChatPDF ğŸ¦–')
st.write('---')

# openapi_key ì…ë ¥ ë°›ê¸°
api_key = st.text_input('OpenAI key', type='password')
st.button('set')

if api_key:
    uploaded_file = st.file_uploader("Please upload the PDF file", type=['pdf'])
    st.write('---')
    if uploaded_file is not None:
        pages = pdf_to_document(uploaded_file)

        # text ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            #set a really small chunk size, just to show.
            chunk_size=100, # ê° chunkì˜ ìµœëŒ€ ê¸¸ì´
            chunk_overlap=20, # ì¸ì ‘í•œ chunk ì‚¬ì´ì˜ ì¤‘ë³µ ì˜ì—­, ë¬¸ì¥ì´ ëŠê¸°ëŠ” ë¬¸ì œë¥¼ í•´ê²° í•˜ê¸° ìœ„í•´ 20ê¸€ì ê²¹ì¹¨
            length_function=len, # chunk ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜
            is_separator_regex=False,  # ë‹¨ìˆœí•œ ë¬¸ìì—´ë¡œ í•´ì„
        )
        texts = text_splitter.split_documents(pages)

        # ì„ë² ë”© (OpenAi key ì‚¬ìš©)
        embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)

        # Chroma vector DB
        db = Chroma.from_documents(texts, embeddings_model)
        # ë°°í¬ì‹œ---
        # import chromadb
        # chromadb.api.client.SharedSystemClient.clear_system_cache()
        #----   
        
        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬í•  Handler ìƒì„±
        class StreamHandler(BaseCallbackHandler):
            def __init__(self, container, initial_text=""):
                self.container = container
                self.text=initial_text
            def on_llm_new_token(self, token: str, **kwargs) -> None:
                self.text+=token
                self.container.markdown(self.text)


        st.header('Your question')
        question = st.text_input('input')

        if st.button('run'):
            with st.spinner("Wait for it...", show_time=True):
                llm = ChatOpenAI(api_key=api_key, temperature=0)

                # Chroma ë°±í„° ì €ì¥ì†Œì— ëŒ€í•œ Retriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(),
                                                                  llm=llm)

                #prompt template
                prompt = hub.pull('rlm/rag-prompt')

                #ì¶œë ¥ê³µê°„ í™•ë³´ stream ë¶€ë¶„
                chat_box = st.empty()
                stream_handler = StreamHandler(chat_box)
                generate_llm = ChatOpenAI(model="gpt-4o-mini",temperature=0, openai_api_key=api_key, streaming=True, callbacks=[stream_handler])
      
                #generate (ê²€ìƒ‰ê²°ê³¼ format)
                def format_docs(docs):
                    return '\n\n'.join(doc.page_content for doc in docs)

                rag_chain = (
                    {'context':retriever_from_llm | format_docs, "question":RunnablePassthrough()}
                    | prompt
                    | generate_llm
                    | llm
                    | StrOutputParser()
                )

                #question
                result = rag_chain.invoke(question)
                st.write(result)

    

